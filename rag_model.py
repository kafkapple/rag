from langchain_openai import ChatOpenAI
import time

class RAGModel:
    """LM Studio APIë¥¼ í†µí•´ LLaMA, Qwen, DeepSeek ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, api_url: str):
        print("ğŸ”„ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.llm = ChatOpenAI(
            base_url=api_url,
            api_key="not-needed",
            temperature=0.7,
            max_tokens=512,
            request_timeout=30  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
        )
        print("âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def generate_answer(self, query, retrieved_docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        max_context_length = 3000  # ì ì ˆí•œ ê°’ìœ¼ë¡œ ì¡°ì •
        context = "\n".join([doc.page_content[:max_context_length] for doc in retrieved_docs])
        
        prompt = (
            "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
            f"ë¬¸ì„œ ë‚´ìš©: {context}\n\n"
            f"ì§ˆë¬¸: {query}\n\n"
            "ë‹µë³€:"
        )
        
        try:
            start_time = time.time()
            response = self.llm.invoke(prompt)
            end_time = time.time()
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
            return response.content
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
